# Limitations

While this collection and analysis of student critical thinking performance at the beginning and the end of semester is valuable, it is important to point out the limitations of this approach.

First, obviously, the sample is very small. The fact that the post survey is half the size of the pre survey is also relevant. Had they been the same size, our comparison would be more precise. Additionally, we don't know if *exactly the same students* filled in both surveys. Completion was not mandatory, so it is quite possible that some students filled in the pre but not the post suvey, or vice versa. That produces more uncertainty about the results.

Still, the biggest limitation comes from this kind of research design. T-tests *do not uncover causality*, they are not made for that. They are good enough to make statistically significant conclusions about differences betweent the two means, and that's it. In other words, while we can use the t-test to see if there's difference between the two sets of data, we cannot use it to make conclusions about what produced such difference. It could be the case that something else made students slightly better at critical thinking by the end of the semester (maybe they matured in the meantime, consumed a lot of Omega 3, got bitten by a radiocative spider?), and the CRT classes made no difference. We cannot know.

## Possible extensions

I'd like to continue thinking about this topic and do more research. Besides a larger sample, I'd like to extend it by including a control group - another group of students who are not taking the critical thinking class. This would allow for making some modest causal claims. If we see improvement in the group taking the CRT class and we don't see comparable improvement in the group not taking the class, then perhaps we could say that teaching critical thinking makes difference.